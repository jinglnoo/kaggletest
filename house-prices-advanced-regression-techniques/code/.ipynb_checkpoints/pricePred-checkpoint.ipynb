{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essentials\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import random\n",
    "\n",
    "# Plots\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Models\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor, BaggingRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.linear_model import Ridge, RidgeCV\n",
    "from sklearn.linear_model import ElasticNet, ElasticNetCV\n",
    "from sklearn.svm import SVR\n",
    "from mlxtend.regressor import StackingCVRegressor\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Stats\n",
    "from scipy.stats import skew, norm\n",
    "from scipy.special import boxcox1p\n",
    "from scipy.stats import boxcox_normmax\n",
    "\n",
    "# Misc\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\")\n",
    "pd.options.display.max_seq_items = 8000\n",
    "pd.options.display.max_rows = 8000\n",
    "\n",
    "train = pd.read_csv(r'C://Users/jingl/Downloads/house-prices-advanced-regression-techniques/train.csv')\n",
    "test = pd.read_csv(r'C://Users/jingl/Downloads/house-prices-advanced-regression-techniques/test.csv')\n",
    "\n",
    "#train.head()\n",
    "# Finding numeric features\n",
    "numeric_dtypes = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "numeric = []\n",
    "for i in train.columns:\n",
    "    if train[i].dtype in numeric_dtypes:\n",
    "        if i in ['TotalSF', 'Total_Bathrooms','Total_porch_sf','haspool','hasgarage','hasbsmt','hasfireplace']:\n",
    "            pass\n",
    "        else:\n",
    "            numeric.append(i)    \n",
    "\"\"\"\n",
    "# visualising some more outliers in the data values\n",
    "fig, axs = plt.subplots(ncols=2, nrows=0, figsize=(12, 120))\n",
    "plt.subplots_adjust(right=2)\n",
    "plt.subplots_adjust(top=2)\n",
    "sns.color_palette(\"husl\", 8)\n",
    "for i, feature in enumerate(list(train[numeric]), 1):\n",
    "    if(feature=='MiscVal'):\n",
    "        break\n",
    "    plt.subplot(len(list(numeric)), 3, i)\n",
    "    sns.scatterplot(x=feature, y='SalePrice', hue='SalePrice', palette='Blues', data=train)\n",
    "        \n",
    "    plt.xlabel('{}'.format(feature), size=15,labelpad=12.5)\n",
    "    plt.ylabel('SalePrice', size=15, labelpad=12.5)\n",
    "    \n",
    "    for j in range(2):\n",
    "        plt.tick_params(axis='x', labelsize=12)\n",
    "        plt.tick_params(axis='y', labelsize=12)\n",
    "    \n",
    "    plt.legend(loc='best', prop={'size': 10})\n",
    "        \n",
    "plt.show()\n",
    "\n",
    "corr = train.corr()\n",
    "plt.subplots(figsize=(15,12))\n",
    "sns.heatmap(corr, vmax=0.9, cmap=\"Blues\", square=True)\n",
    "\"\"\"\n",
    "train_ID = train['Id']\n",
    "test_ID = test['Id']\n",
    "train.drop(['Id'], axis=1, inplace=True)\n",
    "test.drop(['Id'], axis=1, inplace=True)\n",
    "# log(1+x) transform\n",
    "train[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])\n",
    "\"\"\"\n",
    "sns.set_style(\"white\")\n",
    "sns.set_color_codes(palette='deep')\n",
    "f, ax = plt.subplots(figsize=(8, 7))\n",
    "#Check the new distribution \n",
    "sns.distplot(train['SalePrice'] , fit=norm, color=\"b\");\n",
    "\n",
    "# Get the fitted parameters used by the function\n",
    "(mu, sigma) = norm.fit(train['SalePrice'])\n",
    "print( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n",
    "\n",
    "#Now plot the distribution\n",
    "plt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n",
    "            loc='best')\n",
    "ax.xaxis.grid(False)\n",
    "ax.set(ylabel=\"Frequency\")\n",
    "ax.set(xlabel=\"SalePrice\")\n",
    "ax.set(title=\"SalePrice distribution\")\n",
    "sns.despine(trim=True, left=True)\n",
    "\n",
    "plt.show()\n",
    "\"\"\"\n",
    "# Remove outliers\n",
    "train.drop(train[(train['OverallQual']<5) & (train['SalePrice']>200000)].index, inplace=True)\n",
    "train.drop(train[(train['GrLivArea']>4500) & (train['SalePrice']<300000)].index, inplace=True)\n",
    "train.reset_index(drop=True, inplace=True)\n",
    "\n",
    "train_labels = train['SalePrice'].reset_index(drop=True)\n",
    "train_features = train.drop(['SalePrice'], axis=1)\n",
    "test_features = test\n",
    "\n",
    "# Combine train and test features in order to apply the feature transformation pipeline to the entire dataset\n",
    "all_features = pd.concat([train_features, test_features]).reset_index(drop=True)\n",
    "\n",
    "def percent_missing(df):\n",
    "    data = pd.DataFrame(df)\n",
    "    df_cols = list(pd.DataFrame(data))\n",
    "    dict_x = {}\n",
    "    for i in range(0, len(df_cols)):\n",
    "        dict_x.update({df_cols[i]: round(data[df_cols[i]].isnull().mean()*100,2)})\n",
    "    \n",
    "    return dict_x\n",
    "\"\"\"\n",
    "missing = percent_missing(all_features)\n",
    "df_miss = sorted(missing.items(), key=lambda x: x[1], reverse=True)\n",
    "print('Percent of missing data')\n",
    "df_miss[0:10]\n",
    "# Visualize missing values\n",
    "sns.set_style(\"white\")\n",
    "f, ax = plt.subplots(figsize=(8, 7))\n",
    "sns.set_color_codes(palette='deep')\n",
    "missing = round(train.isnull().mean()*100,2)\n",
    "missing = missing[missing > 0]\n",
    "missing.sort_values(inplace=True)\n",
    "missing.plot.bar(color=\"b\")\n",
    "# Tweak the visual presentation\n",
    "ax.xaxis.grid(False)\n",
    "ax.set(ylabel=\"Percent of missing values\")\n",
    "ax.set(xlabel=\"Features\")\n",
    "ax.set(title=\"Percent missing data by feature\")\n",
    "sns.despine(trim=True, left=True)\n",
    "\"\"\"\n",
    "# Some of the non-numeric predictors are stored as numbers; convert them into strings \n",
    "all_features['MSSubClass'] = all_features['MSSubClass'].apply(str)\n",
    "all_features['YrSold'] = all_features['YrSold'].astype(str)\n",
    "all_features['MoSold'] = all_features['MoSold'].astype(str)\n",
    "\n",
    "def handle_missing(features):\n",
    "    # the data description states that NA refers to typical ('Typ') values\n",
    "    features['Functional'] = features['Functional'].fillna('Typ')\n",
    "    # Replace the missing values in each of the columns below with their mode\n",
    "    features['Electrical'] = features['Electrical'].fillna(\"SBrkr\")\n",
    "    features['KitchenQual'] = features['KitchenQual'].fillna(\"TA\")\n",
    "    features['Exterior1st'] = features['Exterior1st'].fillna(features['Exterior1st'].mode()[0])\n",
    "    features['Exterior2nd'] = features['Exterior2nd'].fillna(features['Exterior2nd'].mode()[0])\n",
    "    features['SaleType'] = features['SaleType'].fillna(features['SaleType'].mode()[0])\n",
    "    features['MSZoning'] = features.groupby('MSSubClass')['MSZoning'].transform(lambda x: x.fillna(x.mode()[0]))\n",
    "    \n",
    "    # the data description stats that NA refers to \"No Pool\"\n",
    "    features[\"PoolQC\"] = features[\"PoolQC\"].fillna(\"None\")\n",
    "    # Replacing the missing values with 0, since no garage = no cars in garage\n",
    "    for col in ('GarageYrBlt', 'GarageArea', 'GarageCars'):\n",
    "        features[col] = features[col].fillna(0)\n",
    "    # Replacing the missing values with None\n",
    "    for col in ['GarageType', 'GarageFinish', 'GarageQual', 'GarageCond']:\n",
    "        features[col] = features[col].fillna('None')\n",
    "    # NaN values for these categorical basement features, means there's no basement\n",
    "    for col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n",
    "        features[col] = features[col].fillna('None')\n",
    "        \n",
    "    # Group the by neighborhoods, and fill in missing value by the median LotFrontage of the neighborhood\n",
    "    features['LotFrontage'] = features.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.median()))\n",
    "\n",
    "    # We have no particular intuition around how to fill in the rest of the categorical features\n",
    "    # So we replace their missing values with None\n",
    "    objects = []\n",
    "    for i in features.columns:\n",
    "        if features[i].dtype == object:\n",
    "            objects.append(i)\n",
    "    features.update(features[objects].fillna('None'))\n",
    "        \n",
    "    # And we do the same thing for numerical features, but this time with 0s\n",
    "    numeric_dtypes = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    numeric = []\n",
    "    for i in features.columns:\n",
    "        if features[i].dtype in numeric_dtypes:\n",
    "            numeric.append(i)\n",
    "    features.update(features[numeric].fillna(0))    \n",
    "    return features\n",
    "\n",
    "all_features = handle_missing(all_features)\n",
    "# Let's make sure we handled all the missing values\n",
    "missing = percent_missing(all_features)\n",
    "df_miss = sorted(missing.items(), key=lambda x: x[1], reverse=True)\n",
    "#print('Percent of missing data')\n",
    "#df_miss[0:10]\n",
    "\n",
    "# Fetch all numeric features\n",
    "numeric_dtypes = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "numeric = []\n",
    "for i in all_features.columns:\n",
    "    if all_features[i].dtype in numeric_dtypes:\n",
    "        numeric.append(i)\n",
    "\"\"\"\n",
    "# Create box plots for all numeric features\n",
    "sns.set_style(\"white\")\n",
    "f, ax = plt.subplots(figsize=(8, 7))\n",
    "ax.set_xscale(\"log\")\n",
    "ax = sns.boxplot(data=all_features[numeric] , orient=\"h\", palette=\"Set1\")\n",
    "ax.xaxis.grid(False)\n",
    "ax.set(ylabel=\"Feature names\")\n",
    "ax.set(xlabel=\"Numeric values\")\n",
    "ax.set(title=\"Numeric Distribution of Features\")\n",
    "sns.despine(trim=True, left=True)\n",
    "\"\"\"\n",
    "# Find skewed numerical features\n",
    "skew_features = all_features[numeric].apply(lambda x: skew(x)).sort_values(ascending=False)\n",
    "\n",
    "high_skew = skew_features[skew_features > 0.5]\n",
    "skew_index = high_skew.index\n",
    "\n",
    "#print(\"There are {} numerical features with Skew > 0.5 :\".format(high_skew.shape[0]))\n",
    "#skewness = pd.DataFrame({'Skew' :high_skew})\n",
    "#skew_features.head(10)\n",
    "for i in skew_index:\n",
    "    all_features[i] = boxcox1p(all_features[i], boxcox_normmax(all_features[i] + 1))\n",
    "\n",
    "all_features['BsmtFinType1_Unf'] = 1*(all_features['BsmtFinType1'] == 'Unf')\n",
    "all_features['HasWoodDeck'] = (all_features['WoodDeckSF'] == 0) * 1\n",
    "all_features['HasOpenPorch'] = (all_features['OpenPorchSF'] == 0) * 1\n",
    "all_features['HasEnclosedPorch'] = (all_features['EnclosedPorch'] == 0) * 1\n",
    "all_features['Has3SsnPorch'] = (all_features['3SsnPorch'] == 0) * 1\n",
    "all_features['HasScreenPorch'] = (all_features['ScreenPorch'] == 0) * 1\n",
    "all_features['YearsSinceRemodel'] = all_features['YrSold'].astype(int) - all_features['YearRemodAdd'].astype(int)\n",
    "all_features['Total_Home_Quality'] = all_features['OverallQual'] + all_features['OverallCond']\n",
    "all_features = all_features.drop(['Utilities', 'Street', 'PoolQC',], axis=1)\n",
    "all_features['TotalSF'] = all_features['TotalBsmtSF'] + all_features['1stFlrSF'] + all_features['2ndFlrSF']\n",
    "all_features['YrBltAndRemod'] = all_features['YearBuilt'] + all_features['YearRemodAdd']\n",
    "all_features['Total_sqr_footage'] = (all_features['BsmtFinSF1'] + all_features['BsmtFinSF2'] +\n",
    "                                 all_features['1stFlrSF'] + all_features['2ndFlrSF'])\n",
    "all_features['Total_Bathrooms'] = (all_features['FullBath'] + (0.5 * all_features['HalfBath']) +\n",
    "                               all_features['BsmtFullBath'] + (0.5 * all_features['BsmtHalfBath']))\n",
    "all_features['Total_porch_sf'] = (all_features['OpenPorchSF'] + all_features['3SsnPorch'] +\n",
    "                              all_features['EnclosedPorch'] + all_features['ScreenPorch'] +\n",
    "                              all_features['WoodDeckSF'])\n",
    "all_features['TotalBsmtSF'] = all_features['TotalBsmtSF'].apply(lambda x: np.exp(6) if x <= 0.0 else x)\n",
    "all_features['2ndFlrSF'] = all_features['2ndFlrSF'].apply(lambda x: np.exp(6.5) if x <= 0.0 else x)\n",
    "all_features['GarageArea'] = all_features['GarageArea'].apply(lambda x: np.exp(6) if x <= 0.0 else x)\n",
    "all_features['GarageCars'] = all_features['GarageCars'].apply(lambda x: 0 if x <= 0.0 else x)\n",
    "all_features['LotFrontage'] = all_features['LotFrontage'].apply(lambda x: np.exp(4.2) if x <= 0.0 else x)\n",
    "all_features['MasVnrArea'] = all_features['MasVnrArea'].apply(lambda x: np.exp(4) if x <= 0.0 else x)\n",
    "all_features['BsmtFinSF1'] = all_features['BsmtFinSF1'].apply(lambda x: np.exp(6.5) if x <= 0.0 else x)\n",
    "\n",
    "all_features['haspool'] = all_features['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\n",
    "all_features['has2ndfloor'] = all_features['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)\n",
    "all_features['hasgarage'] = all_features['GarageArea'].apply(lambda x: 1 if x > 0 else 0)\n",
    "all_features['hasbsmt'] = all_features['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)\n",
    "all_features['hasfireplace'] = all_features['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)\n",
    "\n",
    "def logs(res, ls):\n",
    "    m = res.shape[1]\n",
    "    for l in ls:\n",
    "        res = res.assign(newcol=pd.Series(np.log(1.01+res[l])).values)   \n",
    "        res.columns.values[m] = l + '_log'\n",
    "        m += 1\n",
    "    return res\n",
    "\n",
    "log_features = ['LotFrontage','LotArea','MasVnrArea','BsmtFinSF1','BsmtFinSF2','BsmtUnfSF',\n",
    "                 'TotalBsmtSF','1stFlrSF','2ndFlrSF','LowQualFinSF','GrLivArea',\n",
    "                 'BsmtFullBath','BsmtHalfBath','FullBath','HalfBath','BedroomAbvGr','KitchenAbvGr',\n",
    "                 'TotRmsAbvGrd','Fireplaces','GarageCars','GarageArea','WoodDeckSF','OpenPorchSF',\n",
    "                 'EnclosedPorch','3SsnPorch','ScreenPorch','PoolArea','MiscVal','YearRemodAdd','TotalSF']\n",
    "all_features = logs(all_features, log_features)\n",
    "\n",
    "def squares(res, ls):\n",
    "    m = res.shape[1]\n",
    "    for l in ls:\n",
    "        res = res.assign(newcol=pd.Series(res[l]*res[l]).values)   \n",
    "        res.columns.values[m] = l + '_sq'\n",
    "        m += 1\n",
    "    return res \n",
    "\n",
    "squared_features = ['YearRemodAdd', 'LotFrontage_log', \n",
    "              'TotalBsmtSF_log', '1stFlrSF_log', '2ndFlrSF_log', 'GrLivArea_log',\n",
    "              'GarageCars_log', 'GarageArea_log']\n",
    "all_features = squares(all_features, squared_features)\n",
    "\n",
    "all_features = pd.get_dummies(all_features).reset_index(drop=True)\n",
    "all_features = all_features.loc[:,~all_features.columns.duplicated()]\n",
    "X = all_features.iloc[:len(train_labels), :]\n",
    "X_test = all_features.iloc[len(train_labels):, :]\n",
    "# Setup cross validation folds\n",
    "kf = KFold(n_splits=12, random_state=42, shuffle=True)\n",
    "# Define error metrics\n",
    "def rmsle(y, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y, y_pred))\n",
    "\n",
    "def cv_rmse(model, X=X):\n",
    "    rmse = np.sqrt(-cross_val_score(model, X, train_labels, scoring=\"neg_mean_squared_error\", cv=kf))\n",
    "    return (rmse)\n",
    "\n",
    "# Light Gradient Boosting Regressor\n",
    "lightgbm = LGBMRegressor(objective='regression', \n",
    "                       num_leaves=6,\n",
    "                       learning_rate=0.01, \n",
    "                       n_estimators=7000,\n",
    "                       max_bin=200, \n",
    "                       bagging_fraction=0.8,\n",
    "                       bagging_freq=4, \n",
    "                       bagging_seed=8,\n",
    "                       feature_fraction=0.2,\n",
    "                       feature_fraction_seed=8,\n",
    "                       min_sum_hessian_in_leaf = 11,\n",
    "                       verbose=-1,\n",
    "                       random_state=42)\n",
    "\n",
    "# XGBoost Regressor\n",
    "xgboost = XGBRegressor(learning_rate=0.01,\n",
    "                       n_estimators=6000,\n",
    "                       max_depth=4,\n",
    "                       min_child_weight=0,\n",
    "                       gamma=0.6,\n",
    "                       subsample=0.7,\n",
    "                       colsample_bytree=0.7,\n",
    "                       objective='reg:linear',\n",
    "                       nthread=-1,\n",
    "                       scale_pos_weight=1,\n",
    "                       seed=27,\n",
    "                       reg_alpha=0.00006,\n",
    "                       random_state=42)\n",
    "\n",
    "# Ridge Regressor\n",
    "ridge_alphas = [1e-15, 1e-10, 1e-8, 9e-4, 7e-4, 5e-4, 3e-4, 1e-4, 1e-3, 5e-2, 1e-2, 0.1, 0.3, 1, 3, 5, 10, 15, 18, 20, 30, 50, 75, 100]\n",
    "ridge = make_pipeline(RobustScaler(), RidgeCV(alphas=ridge_alphas, cv=kf))\n",
    "\n",
    "# Support Vector Regressor\n",
    "svr = make_pipeline(RobustScaler(), SVR(C= 20, epsilon= 0.008, gamma=0.0003))\n",
    "\n",
    "# Gradient Boosting Regressor\n",
    "gbr = GradientBoostingRegressor(n_estimators=6000,\n",
    "                                learning_rate=0.01,\n",
    "                                max_depth=4,\n",
    "                                max_features='sqrt',\n",
    "                                min_samples_leaf=15,\n",
    "                                min_samples_split=10,\n",
    "                                loss='huber',\n",
    "                                random_state=42)  \n",
    "\n",
    "# Random Forest Regressor\n",
    "rf = RandomForestRegressor(n_estimators=1200,\n",
    "                          max_depth=15,\n",
    "                          min_samples_split=5,\n",
    "                          min_samples_leaf=5,\n",
    "                          max_features=None,\n",
    "                          oob_score=True,\n",
    "                          random_state=42)\n",
    "\n",
    "# Stack up all the models above, optimized using xgboost\n",
    "stack_gen = StackingCVRegressor(regressors=(xgboost, lightgbm, svr, ridge, gbr, rf),\n",
    "                                meta_regressor=xgboost,\n",
    "                                use_features_in_secondary=True)\n",
    "scores = {}\n",
    "\n",
    "score = cv_rmse(lightgbm)\n",
    "print(\"lightgbm: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\n",
    "scores['lgb'] = (score.mean(), score.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
